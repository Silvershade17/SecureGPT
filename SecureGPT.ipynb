{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install accelerate\n",
    "!pip install transformers\n",
    "import os\n",
    "!pip install bitsandbytes\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ],
   "id": "ca4fea957adce947"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Api_Key_HG= os.environ.get(\"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\")\n",
    "Name_of_LLM=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "filenames={\n",
    "    \"config.json\",\"generation_config.json\",\"model-00001-of-00003.safetensors\",\"model-00002-of-00003.safetensors\",\"model-00003-of-00003.safetensors\",\"model.safetensors.index.json\",\"pytorch_model-00001-of-00003.bin\",\"pytorch_model-00002-of-00003.bin\",\"pytorch_model-00003-of-00003.bin\",\"pytorch_model.bin.index.json\",\"special_tokens_map.json\",\"tokenizer.json\",\"tokenizer.model\",\"tokenizer_config.json\"\n",
    "}\n",
    "\n",
    "for filename in filenames:\n",
    "    download_model_path=hf_hub_download(\n",
    "        repo_id=Name_of_LLM,\n",
    "        filename=filename,\n",
    "        token=Api_Key_HG\n",
    "    )\n",
    "    print(download_model_path)"
   ],
   "id": "f5b1eb93568c2631"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "my_Token = AutoTokenizer.from_pretrained(Name_of_LLM)\n",
    "BitsnBytesconf = BitsAndBytesConfig(bnb_4bit_use_double_quant=True,bnb_4bit_quant_type=\"nf4\",load_in_4bit=True)"
   ],
   "id": "7b647873a5cbc6f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainedmodle = AutoModelForCausalLM.from_pretrained(\n",
    "        Name_of_LLM,\n",
    "        quantization_config=BitsnBytesconf,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )"
   ],
   "id": "b4111375cf141f4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "p = pipeline(\"text-generation\",model=trainedmodle,torch_dtype=torch.bfloat16,tokenizer = my_Token,device_map=\"auto\")",
   "id": "2081d0c8bb989054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "My_Promt = input(\"Enter Prompt\")\n",
    "\n",
    "sqn = p(\n",
    "    My_Promt,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    ")"
   ],
   "id": "3dc9dc99d7f1f5ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(sqn[0]['generated_text'])",
   "id": "2caee6d722c4b7ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
